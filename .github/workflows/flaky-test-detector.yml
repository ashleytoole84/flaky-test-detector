name: Flaky Test Detector

on:
  workflow_call:
    inputs:
      repository:
        description: 'Repository to analyze (e.g., <org-name>/<repo>)'
        required: true
        type: string
      workflow-name:
        description: 'Name of the workflow to analyze (e.g., CI)'
        required: true
        type: string
      runs-limit:
        description: 'Number of previous runs to analyze'
        default: 10
        type: number
      branch:
        description: 'Branch to filter runs (e.g., main)'
        default: 'main'
        type: string
    secrets:
      github-token:
        description: 'GitHub PAT with repo and workflow scopes'
        required: true
    outputs:
      flaky-tests:
        description: 'JSON string of flaky tests and their failure counts'
        value: ${{ jobs.detect-flaky-tests.outputs.flaky-tests }}
      all-tests:
        description: 'JSON string of all test results (pass/fail counts)'
        value: ${{ jobs.detect-flaky-tests.outputs.all-tests }}

jobs:
  detect-flaky-tests:
    runs-on: ubuntu-latest
    outputs:
      flaky-tests: ${{ steps.process-results.outputs.flaky-tests }}
      all-tests: ${{ steps.process-results.outputs.all-tests }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: pip install requests

      - name: Fetch previous workflow runs
        id: fetch-runs
        env:
          GITHUB_TOKEN: ${{ secrets.github-token }}
          REPO: ${{ inputs.repository }}
          WORKFLOW_NAME: ${{ inputs.workflow-name }}
          RUNS_LIMIT: ${{ inputs.runs-limit }}
          BRANCH: ${{ inputs.branch }}
        run: |
          python - <<EOF
          import requests
          import json

          headers = {"Authorization": f"Bearer $GITHUB_TOKEN", "Accept": "application/vnd.github.v3+json"}
          url = f"https://api.github.com/repos/$REPO/actions/runs?per_page=$RUNS_LIMIT&branch=$BRANCH"
          print(f"Fetching runs from: {url}")
          response = requests.get(url, headers=headers)
          if response.status_code != 200:
              print(f"Error fetching runs: {response.status_code} {response.text}")
              exit(1)
          runs = response.json()["workflow_runs"]
          print(f"Total runs found: {len(runs)}")
          print("All run names:", [run["name"] for run in runs])
          runs = [run for run in runs if run["name"] == "$WORKFLOW_NAME"]
          run_ids = [run["id"] for run in runs]
          print(f"Filtered {len(run_ids)} runs for workflow '$WORKFLOW_NAME': {run_ids}")
          with open("run_ids.txt", "w") as f:
              json.dump(run_ids, f)
          EOF

      - name: Fetch and parse test artifacts
        id: process-results
        env:
          GITHUB_TOKEN: ${{ secrets.github-token }}
          REPO: ${{ inputs.repository }}
        run: |
          python - <<EOF
          import requests
          import json
          import xml.etree.ElementTree as ET
          import zipfile
          import os
          import glob

          headers = {"Authorization": f"Bearer $GITHUB_TOKEN", "Accept": "application/vnd.github.v3+json"}
          with open("run_ids.txt") as f:
              run_ids = json.load(f)
          if not run_ids:
              print("No run IDs found. Check workflow name and branch.")
              print(f"all-tests={{}}", file=open("$GITHUB_OUTPUT", "a"))
              print(f"flaky-tests={{}}", file=open("$GITHUB_OUTPUT", "a"))
              exit(0)

          test_results = {}
          for run_id in run_ids:
              print(f"Processing run ID: {run_id}")
              run_url = f"https://api.github.com/repos/$REPO/actions/runs/{run_id}"
              run_response = requests.get(run_url, headers=headers)
              if run_response.status_code != 200:
                  print(f"Error fetching run {run_id}: {run_response.status_code} {run_response.text}")
                  continue
              run_data = run_response.json()
              run_attempts = run_data.get("run_attempt", 1)
              print(f"Run {run_id} has {run_attempts} attempt(s) reported, will fetch artifacts for each")

              # Fetch artifacts for each attempt
              for attempt in range(1, run_attempts + 1):
                  print(f"Processing attempt {attempt} for run {run_id}")
                  artifacts_url = f"https://api.github.com/repos/$REPO/actions/runs/{run_id}/artifacts"
                  artifacts_response = requests.get(artifacts_url, headers=headers)
                  if artifacts_response.status_code != 200:
                      print(f"Error fetching artifacts for run {run_id}, attempt {attempt}: {artifacts_response.status_code} {artifacts_response.text}")
                      continue
                  artifacts = artifacts_response.json()["artifacts"]
                  print(f"Found {len(artifacts)} artifacts for run {run_id}, attempt {attempt}: {[a['name'] for a in artifacts]}")
                  for artifact in artifacts:
                      if artifact["name"] == "test-results":
                          zip_url = artifact["archive_download_url"]
                          print(f"Downloading artifact from {zip_url}")
                          zip_response = requests.get(zip_url, headers=headers)
                          if zip_response.status_code != 200:
                              print(f"Error downloading artifact {zip_url}: {zip_response.status_code} {zip_response.text}")
                              continue
                          with open(f"artifacts-run-{run_id}-attempt-{attempt}.zip", "wb") as f:
                              f.write(zip_response.content)
                          with zipfile.ZipFile(f"artifacts-run-{run_id}-attempt-{attempt}.zip", "r") as zip_ref:
                              zip_ref.extractall(f"artifacts/run-{run_id}-attempt-{attempt}")
                          all_files = []
                          for root, _, files in os.walk(f"artifacts/run-{run_id}-attempt-{attempt}"):
                              for f in files:
                                  all_files.append(os.path.join(root, f))
                          print(f"All files in artifact for run {run_id}, attempt {attempt}: {all_files}")
                          xml_files = glob.glob(f"artifacts/run-{run_id}-attempt-{attempt}/**/TEST-*.xml", recursive=True)
                          print(f"Found {len(xml_files)} XML files for run {run_id}, attempt {attempt}: {xml_files}")
                          for file_path in xml_files:
                              print(f"Parsing XML file: {file_path}")
                              try:
                                  tree = ET.parse(file_path)
                                  root = tree.getroot()
                                  print(f"XML root tag: {root.tag}, attributes: {root.attrib}")
                                  for testcase in root.findall(".//testcase"):
                                      name = testcase.get("name")
                                      classname = testcase.get("classname", "")
                                      full_name = f"{classname}.{name}" if classname else name
                                      if not full_name:
                                          print(f"Warning: Testcase without name/classname in {file_path}")
                                          continue
                                      if testcase.find("failure") is not None or testcase.find("error") is not None:
                                          test_results.setdefault(full_name, {"pass": 0, "fail": 0})["fail"] += 1
                                          print(f"Test {full_name} failed in {file_path}")
                                      else:
                                          test_results.setdefault(full_name, {"pass": 0, "fail": 0})["pass"] += 1
                                          print(f"Test {full_name} passed in {file_path}")
                              except ET.ParseError as e:
                                  print(f"Error parsing XML file {file_path}: {e}")
                                  continue

          print("All test results:")
          if test_results:
              for test, counts in test_results.items():
                  print(f"- {test}: {counts['pass']} passes, {counts['fail']} failures")
          else:
              print("No test results parsed.")
          flaky_tests = {k: v["fail"] for k, v in test_results.items() if v["pass"] > 0 and v["fail"] > 0}
          print("Flaky tests:")
          if flaky_tests:
              for test, fails in flaky_tests.items():
                  print(f"- {test}: failed {fails} out of {test_results[test]['pass'] + test_results[test]['fail']} runs")
          else:
              print("No flaky tests detected.")
          with open("flaky-tests.json", "w") as f:
              json.dump(flaky_tests, f)
          with open("all-tests.json", "w") as f:
              json.dump(test_results, f)
          # Ensure valid JSON output
          flaky_tests_json = json.dumps(flaky_tests)
          all_tests_json = json.dumps(test_results)
          print(f"flaky-tests={flaky_tests_json}", file=open("$GITHUB_OUTPUT", "a"))
          print(f"all-tests={all_tests_json}", file=open("$GITHUB_OUTPUT", "a"))
          EOF